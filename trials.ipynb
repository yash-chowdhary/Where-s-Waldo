{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import cyvlfeat as vlfeat\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "import os.path as osp\n",
    "from skimage import filters\n",
    "from skimage.feature import corner_peaks\n",
    "from skimage.io import imread\n",
    "import pickle\n",
    "from random import shuffle\n",
    "from scipy.spatial.distance import cdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bags_of_sifts(image_paths=[], vocab_filename='', img=None):\n",
    "\n",
    "    with open(vocab_filename, 'rb') as f:\n",
    "        vocab = pickle.load(f)\n",
    "\n",
    "    vocab_size = vocab.shape[0]\n",
    "    feats = []\n",
    "    \n",
    "    if len(image_paths) == 0:\n",
    "        image_paths = ['']\n",
    "    \n",
    "    for path in image_paths:\n",
    "        image = np.asarray(plt.imread(path)) if path != '' else img\n",
    "        img_gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "        (frames, descriptors) = vlfeat.sift.dsift(img_gray, fast=True, step=10)\n",
    "        samples = descriptors\n",
    "        D = cdist(samples, vocab)\n",
    "        closest_words = np.argmin(D, axis=1)\n",
    "        histogram, bin_edges = np.histogram(closest_words, bins=np.arange(0, vocab_size+1))    \n",
    "        histogram = histogram / np.linalg.norm(histogram)\n",
    "        feats.append(histogram)\n",
    "        \n",
    "    N = len(image_paths)\n",
    "    d = vocab_size\n",
    "    feats = np.asarray(feats)\n",
    "    feats = feats.reshape((N,d))\n",
    "    return feats\n",
    "\n",
    "\n",
    "def build_vocabulary(image_paths, vocab_size):\n",
    "    dim = 128     \n",
    "    vocab = np.zeros((vocab_size,dim))\n",
    "    sift_features = []\n",
    "\n",
    "    for path in image_paths:\n",
    "        image = np.asarray(plt.imread(path))\n",
    "        img_gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "        (frames, descriptors) = vlfeat.sift.dsift(img_gray, fast=True, step=5)\n",
    "\n",
    "        samples = np.random.permutation(descriptors)[:20]\n",
    "\n",
    "        for descriptor in samples:\n",
    "            sift_features.append(descriptor)\n",
    "\n",
    "    sift_features = np.asarray(sift_features).astype('float64').reshape((-1,128))\n",
    "    vocab = vlfeat.kmeans.kmeans(sift_features, vocab_size)\n",
    "\n",
    "    return vocab\n",
    "\n",
    "\n",
    "def test_accuracy(test_labels, predicted_labels):\n",
    "    num_correct = 0\n",
    "    for i,label in enumerate(test_labels):\n",
    "        if (predicted_labels[i] == label):\n",
    "            num_correct += 1\n",
    "    return num_correct/len(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "waldo_paths = []\n",
    "wenda_paths = []\n",
    "wizard_paths = []\n",
    "all_paths = []\n",
    "\n",
    "test_image_paths = []\n",
    "with open('datasets/ImageSets/val.txt') as file:\n",
    "    for img_id in file.readlines():\n",
    "        img_id = img_id.rstrip()\n",
    "        test_image_paths.append('datasets/JPEGImages/{}.jpg'.format(img_id))\n",
    "\n",
    "file.close()\n",
    "\n",
    "\n",
    "template_dirs = [\"templates/waldo\",\"templates/wenda\",\"templates/wizard\"]\n",
    "\n",
    "for i in range(len(template_dirs)):\n",
    "    for img_id in os.listdir(template_dirs[i]):\n",
    "        path_to_dir = os.path.join(template_dirs[i], '{}'.format(img_id)).rstrip()\n",
    "        if not os.path.isdir(path_to_dir):\n",
    "            continue\n",
    "        list_of_files = os.listdir(path_to_dir)\n",
    "        for file_name in list_of_files:\n",
    "            all_paths.append(os.path.join(path_to_dir, '{}'.format(file_name)).rstrip())\n",
    "            if i==0:\n",
    "                waldo_paths.append(os.path.join(path_to_dir, '{}'.format(file_name)).rstrip())\n",
    "            if i==1:\n",
    "                wenda_paths.append(os.path.join(path_to_dir, '{}'.format(file_name)).rstrip())\n",
    "            if i==2:\n",
    "                wizard_paths.append(os.path.join(path_to_dir, '{}'.format(file_name)).rstrip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_filename = 'vocab.pkl'\n",
    "vocab_size = 200\n",
    "vocab = build_vocabulary(all_paths,vocab_size)\n",
    "    \n",
    "with open(vocab_filename, 'wb') as f:\n",
    "    pickle.dump(vocab, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
