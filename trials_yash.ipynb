{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import cyvlfeat as vlfeat\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "import os.path as osp\n",
    "from skimage import filters\n",
    "from skimage.feature import corner_peaks\n",
    "from skimage.io import imread\n",
    "import pickle\n",
    "from random import shuffle\n",
    "from scipy.spatial.distance import cdist\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bags_of_sifts(image_paths=[], vocab_filename='', img=None):\n",
    "\n",
    "    with open(vocab_filename, 'rb') as f:\n",
    "        vocab = pickle.load(f)\n",
    "\n",
    "    vocab_size = vocab.shape[0]\n",
    "#     print(vocab_size)\n",
    "    feats = []\n",
    "    \n",
    "    if len(image_paths) == 0:\n",
    "        image_paths = ['']\n",
    "    \n",
    "    for path in image_paths:\n",
    "        image = np.asarray(plt.imread(path)) if path != '' else img\n",
    "        img_gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "        (frames, descriptors) = vlfeat.sift.dsift(img_gray, fast=True, step=10)\n",
    "        \n",
    "        samples = descriptors\n",
    "        \n",
    "        D = cdist(samples, vocab)\n",
    "        \n",
    "        closest_words = np.argmin(D, axis=1)\n",
    "\n",
    "#         for i in range(D.shape[0]):\n",
    "#             min_index = np.argmin(D[i])\n",
    "#             histogram[min_index]+=1\n",
    "\n",
    "        histogram, bin_edges = np.histogram(closest_words, bins=np.arange(0, vocab_size+1))\n",
    "#         print(histogram)\n",
    "\n",
    "        if np.linalg.norm(histogram) == 0.0:\n",
    "            print(descriptors.shape)\n",
    "            print(D.shape)\n",
    "            print(np.linalg.norm(histogram))\n",
    "            print(img_gray.shape)\n",
    "            print(\"--\")\n",
    "            \n",
    "            \n",
    "        histogram = histogram / np.linalg.norm(histogram)\n",
    "        feats.append(histogram)\n",
    "        \n",
    "    N = len(image_paths)\n",
    "    d = vocab_size\n",
    "    feats = np.asarray(feats)\n",
    "    feats = feats.reshape((N,d))\n",
    "    return feats\n",
    "\n",
    "\n",
    "def build_vocabulary(image_paths, vocab_size):\n",
    "    dim = 128     \n",
    "    vocab = np.zeros((vocab_size,dim))\n",
    "    sift_features = []\n",
    "\n",
    "    for path in image_paths:\n",
    "        image = np.asarray(plt.imread(path))\n",
    "        img_gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "        (frames, descriptors) = vlfeat.sift.dsift(img_gray, fast=True, step=5)\n",
    "\n",
    "        samples = np.random.permutation(descriptors)[:20]\n",
    "\n",
    "        for descriptor in samples:\n",
    "            sift_features.append(descriptor)\n",
    "\n",
    "    sift_features = np.asarray(sift_features).astype('float64').reshape((-1,128))\n",
    "    vocab = vlfeat.kmeans.kmeans(sift_features, vocab_size)\n",
    "\n",
    "    return vocab\n",
    "\n",
    "def svm_classify(train_image_feats, train_labels, test_image_feats):\n",
    "    categories = list(set(train_labels))\n",
    "    test_labels = []\n",
    "    clf = LinearSVC(C=2)\n",
    "    clf.fit(train_image_feats, train_labels)\n",
    "    test_labels = clf.predict(test_image_feats)\n",
    "\n",
    "    return test_labels\n",
    "\n",
    "\n",
    "def test_accuracy(test_labels, predicted_labels):\n",
    "    num_correct = 0\n",
    "    for i,label in enumerate(test_labels):\n",
    "        if (predicted_labels[i] == label):\n",
    "            num_correct += 1\n",
    "    return num_correct/len(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124\n",
      "36\n",
      "24\n",
      "184\n"
     ]
    }
   ],
   "source": [
    "waldo_paths = []\n",
    "wenda_paths = []\n",
    "wizard_paths = []\n",
    "negative_paths = []\n",
    "all_paths = []\n",
    "\n",
    "test_image_paths = []\n",
    "with open('datasets/ImageSets/val.txt') as file:\n",
    "    for img_id in file.readlines():\n",
    "        img_id = img_id.rstrip()\n",
    "        test_image_paths.append('datasets/JPEGImages/{}.jpg'.format(img_id))\n",
    "\n",
    "file.close()\n",
    "\n",
    "\n",
    "template_dirs = [\"templates/waldo\",\"templates/wenda\",\"templates/wizard\"]\n",
    "\n",
    "for i in range(len(template_dirs)):\n",
    "    for img_id in os.listdir(template_dirs[i]):\n",
    "        path_to_dir = os.path.join(template_dirs[i], '{}'.format(img_id)).rstrip()\n",
    "        if not os.path.isdir(path_to_dir):\n",
    "            continue\n",
    "        list_of_files = os.listdir(path_to_dir)\n",
    "        for file_name in list_of_files:\n",
    "            all_paths.append(os.path.join(path_to_dir, '{}'.format(file_name)).rstrip())\n",
    "            if i==0:\n",
    "                waldo_paths.append(os.path.join(path_to_dir, '{}'.format(file_name)).rstrip())\n",
    "            if i==1:\n",
    "                wenda_paths.append(os.path.join(path_to_dir, '{}'.format(file_name)).rstrip())\n",
    "            if i==2:\n",
    "                wizard_paths.append(os.path.join(path_to_dir, '{}'.format(file_name)).rstrip())\n",
    "\n",
    "negative_dir = \"negatives_same_scale\"\n",
    "\n",
    "for file_name in os.listdir(negative_dir):\n",
    "    path = os.path.join(negative_dir, '{}'.format(file_name)).rstrip()\n",
    "    negative_paths.append(path)\n",
    "    all_paths.append(path)\n",
    "\n",
    "# print(sorted(all_paths))\n",
    "print(len(waldo_paths))\n",
    "print(len(wenda_paths))\n",
    "print(len(wizard_paths))\n",
    "print(len(negative_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the BAG-OF-SIFT representation for images\n",
      "False\n",
      "vocab.pkl saved\n"
     ]
    }
   ],
   "source": [
    "# get vocab\n",
    "print('Using the BAG-OF-SIFT representation for images')\n",
    "\n",
    "vocab_filename = 'vocab.pkl'\n",
    "\n",
    "# print('No existing visual word vocabulary found. Computing one from training images')\n",
    "vocab_size = 200  # Larger values will work better (to a point) but be slower to compute\n",
    "vocab = build_vocabulary(all_paths,vocab_size)\n",
    "print(np.isnan(vocab).any())\n",
    "    \n",
    "with open(vocab_filename, 'wb') as f:\n",
    "    pickle.dump(vocab, f)\n",
    "\n",
    "    print('{:s} saved'.format(vocab_filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_feats = 368\n",
      "--\n",
      "test_feats_lengths\n",
      "--\n",
      "test_labels lengths\n",
      "75\n",
      "293\n",
      "293\n",
      "--\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "#get bags of sifts\n",
    "template_percentage = 0.8\n",
    "\n",
    "print(f\"all_feats = {len(all_paths)}\")\n",
    "\n",
    "# waldo_feats = bags_of_sifts(waldo_paths,vocab_filename)\n",
    "# wenda_feats = bags_of_sifts(wenda_paths,vocab_filename)\n",
    "# wizard_feats = bags_of_sifts(wizard_paths,vocab_filename)\n",
    "\n",
    "# print(np.isnan(waldo_feats).any())\n",
    "# print(np.isnan(wenda_feats).any())\n",
    "# print(np.isnan(wizard_feats).any())\n",
    "\n",
    "# print(waldo_feats)\n",
    "# print(wenda_feats)\n",
    "# print(wizard_feats)\n",
    "\n",
    "\n",
    "waldo_feats = bags_of_sifts(waldo_paths[:int(len(waldo_paths)*template_percentage)],vocab_filename)\n",
    "wenda_feats = bags_of_sifts(wenda_paths[:int(len(wenda_paths)*template_percentage)],vocab_filename)\n",
    "wizard_feats = bags_of_sifts(wizard_paths[:int(len(wizard_paths)*template_percentage)],vocab_filename)\n",
    "negative_feats = bags_of_sifts(negative_paths[:int(len(negative_paths)*template_percentage)], vocab_filename)\n",
    "\n",
    "training_feats = []\n",
    "training_feats.extend(waldo_feats)\n",
    "training_feats.extend(wenda_feats)\n",
    "training_feats.extend(wizard_feats)\n",
    "training_feats.extend(negative_feats)\n",
    "\n",
    "# print(len(waldo_feats))\n",
    "# print(len(wenda_feats))\n",
    "# print(len(wizard_feats))\n",
    "print(\"--\\ntest_feats_lengths\")\n",
    "\n",
    "# test_image_feats \n",
    "waldo_test_feats = bags_of_sifts(waldo_paths[int(len(waldo_paths)*template_percentage):len(waldo_paths)],vocab_filename)\n",
    "wenda_test_feats = bags_of_sifts(wenda_paths[int(len(wenda_paths)*template_percentage):len(wenda_paths)],vocab_filename)\n",
    "wizard_test_feats = bags_of_sifts(wizard_paths[int(len(wizard_paths)*template_percentage):len(wizard_paths)],vocab_filename)\n",
    "negative_test_feats = bags_of_sifts(negative_paths[int(len(negative_paths)*template_percentage):len(negative_paths)],vocab_filename)\n",
    "\n",
    "test_feats = []\n",
    "test_feats.extend(waldo_test_feats)\n",
    "test_feats.extend(wenda_test_feats)\n",
    "test_feats.extend(wizard_test_feats)\n",
    "test_feats.extend(negative_test_feats)\n",
    "\n",
    "#set training labels\n",
    "train_labels = []\n",
    "train_labels.extend([0]*len(waldo_feats))\n",
    "train_labels.extend([1]*len(wenda_feats))\n",
    "train_labels.extend([2]*len(wizard_feats))\n",
    "train_labels.extend([3]*len(negative_feats))\n",
    "\n",
    "# print(len(waldo_test_feats))\n",
    "# print(len(wenda_test_feats))\n",
    "# print(len(wizard_test_feats))\n",
    "print(\"--\\ntest_labels lengths\")\n",
    "\n",
    "ground_truth_test_labels = []\n",
    "ground_truth_test_labels.extend([0]*len(waldo_test_feats))\n",
    "ground_truth_test_labels.extend([1]*len(wenda_test_feats))\n",
    "ground_truth_test_labels.extend([2]*len(wizard_test_feats))\n",
    "ground_truth_test_labels.extend([3]*len(negative_test_feats))\n",
    "\n",
    "print(len(ground_truth_test_labels))\n",
    "\n",
    "print(len(train_labels))\n",
    "print(len(training_feats))\n",
    "print(\"--\")\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def svm_classify(train_image_feats, train_labels, test_image_feats):\n",
    "    categories = list(set(train_labels))\n",
    "    test_labels = []\n",
    "    clf = LinearSVC(C=2)\n",
    "    clf.fit(train_image_feats, train_labels)\n",
    "    test_labels = clf.predict(test_image_feats)\n",
    "\n",
    "    return test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 0 3 1 3 0 0 2 0 2 0 0 3 0 0 0 0 1 3 1 0 0 0 3 0 3 0 3 1 0 0 0 0 2 0 0 3\n",
      " 2 3 3 3 2 3 3 3 3 3 3 0 3 1 3 1 1 0 3 0 0 0 3 3 3 2 3 3 3 3 3 3 0 3 3 2 3\n",
      " 3]\n",
      "0.56\n"
     ]
    }
   ],
   "source": [
    "predicted_svm = svm_classify(training_feats, train_labels, test_feats)\n",
    "print(predicted_svm)\n",
    "print(test_accuracy(ground_truth_test_labels, predicted_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 3 0 0 0 0 0 0 3 0 0 0 3 2 0 0 0 3 1 3 0 2 3 0 3 0 3 1 0 1 0 0 3 0 2 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1 1 3 0 3 3 3 3 3 3 2 3 3 3 3 3 3 3 3 3 2 3\n",
      " 3]\n",
      "0.68\n"
     ]
    }
   ],
   "source": [
    "model = XGBClassifier(learning_rate=0.01, \n",
    "                      n_estimators=2000,\n",
    "                      objective='multi:softprob',\n",
    "                      max_depth=3, \n",
    "                      subsample=0.8, \n",
    "                      colsample_bytree=1,\n",
    "                      num_class=4,\n",
    "                      n_jobs=20)\n",
    "model.fit(np.asarray(training_feats), np.asarray(train_labels))\n",
    "predicted_xgb = model.predict(np.asarray(test_feats))\n",
    "print(predicted_xgb)\n",
    "print(test_accuracy(ground_truth_test_labels, predicted_xgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 3 0 0 0 0 0 0 3 0 0 0 3 2 0 0 0 3 1 3 0 2 3 0 3 0 3 1 0 1 0 0 3 0 2 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1 1 3 0 3 3 3 3 3 3 2 3 3 3 3 3 3 3 3 3 2 3\n",
      " 3]\n",
      "0.68\n"
     ]
    }
   ],
   "source": [
    "model = XGBClassifier(learning_rate=0.01, \n",
    "                      n_estimators=2000,\n",
    "                      max_depth=3, \n",
    "                      subsample=0.8, \n",
    "                      colsample_bytree=1,\n",
    "                      n_jobs=20)\n",
    "model.fit(np.asarray(training_feats), np.asarray(train_labels))\n",
    "predicted_xgb = model.predict(np.asarray(test_feats))\n",
    "print(predicted_xgb)\n",
    "print(test_accuracy(ground_truth_test_labels, predicted_xgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'kp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-959ac84f0a1f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m \u001b[0msliding_window\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m600\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-46-959ac84f0a1f>\u001b[0m in \u001b[0;36msliding_window\u001b[0;34m(window_x, window_y, step_size)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mct\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m             \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'kp' is not defined"
     ]
    }
   ],
   "source": [
    "def svm_probability(train_image_feats, train_labels, test_image_feats):\n",
    "    categories = list(set(train_labels))\n",
    "    test_labels = []\n",
    "    \n",
    "    clf = SVC(C=2, gamma='scale',probability=True)\n",
    "    clf.fit(train_image_feats, train_labels)\n",
    "    test_probabilities = clf.predict_proba(test_image_feats)\n",
    "\n",
    "    return test_probabilities\n",
    "\n",
    "def sliding_window(window_x=200, window_y=600, step_size=1):\n",
    "    f = open('datasets/ImageSets/val.txt')\n",
    "    wa = open('my_waldo.txt', 'w+')\n",
    "    we = open('my_wenda.txt', 'w+')\n",
    "    wi = open('my_wizard.txt', 'w+')\n",
    "    \n",
    "    image_id = f.readline().rstrip()\n",
    "    image_id = '002'\n",
    "    while image_id:\n",
    "        image = np.asarray(plt.imread('datasets/JPEGImages/' + image_id + '.jpg'))\n",
    "        height, width, c = image.shape\n",
    "        \n",
    "        test_feats = []\n",
    "        \n",
    "#         print((height-window_size) * (width-window_size))\n",
    "#         print(f\"{height},{width}\")\n",
    "        \n",
    "        #get the keypoints of the image\n",
    "        #loop through these keypoints only => saves computation time\n",
    "        mser = cv2.MSER_create()\n",
    "#         img_gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "        reg = mser.detectRegions(image)\n",
    "        print(len(reg))\n",
    "        \n",
    "        ct=0\n",
    "        for idx in range(len(kp)):\n",
    "            j,i = kp[idx].pt\n",
    "\n",
    "            i = int(np.round(i))\n",
    "            j = int(np.round(j))\n",
    "#             print(f\"{ct}. {i} {j}\")\n",
    "            \n",
    "            i_limit = i+window_y\n",
    "            j_limit = j+window_x\n",
    "            if i_limit >= height:\n",
    "                i_limit = height-1\n",
    "            if j_limit >= width:\n",
    "                j_limit = width-1\n",
    "            img = image[i:i_limit,j:j_limit]\n",
    "#             print(f\"{i}:{i_limit} {j}:{j_limit}\")\n",
    "            feats = bags_of_sifts(vocab_filename=vocab_filename, img=img)\n",
    "            test_feats.extend(feats)\n",
    "            ct += 1\n",
    "\n",
    "#         print(test_feats)\n",
    "#         print(\"--\")\n",
    "        \n",
    "        #hilda's code: sliding window along entire image\n",
    "#         test_feats=[]\n",
    "#         for i in range(0, height-window_size, step_size):\n",
    "#             for j in range(0, width-window_size, step_size):\n",
    "#                 img = image[i:i+window_size, j:j+window_size]\n",
    "#                 feats = bags_of_sifts(vocab_filename=vocab_filename, img=img)\n",
    "#                 test_feats.extend(feats)\n",
    "#                 if j==1: \n",
    "#                     break\n",
    "#             break\n",
    "\n",
    "#         print(len(test_feats))\n",
    "        predicted_probabilities = model.predict_proba(np.asarray(test_feats))\n",
    "#         print(predicted_probabilities)\n",
    "        locations = np.argmax(predicted_probabilities, axis=0)\n",
    "#         print(locations)\n",
    "        # hilda's code\n",
    "        conf = np.max(predicted_probabilities, axis=0)\n",
    "#         print(conf)\n",
    "\n",
    "        pl = model.predict(np.asarray(test_feats))\n",
    "        locations = np.where(pl == 0)[0]\n",
    "        \n",
    "        for k in range(len(locations)):\n",
    "            #hilda's code\n",
    "#             i = locations[k] // (height-window_size)\n",
    "#             j = locations[k] % (width-window_size)\n",
    "            j_new, i_new  = kp[locations[k]].pt  # x location of best-fit window of character k\n",
    "            \n",
    "            i_new = int(np.round(i_new))\n",
    "            j_new = int(np.round(j_new))\n",
    "#             print(f\"{locations[k]}. {i_new} {j_new}\")\n",
    "            i_limit_new = i_new+window_y\n",
    "            j_limit_new = j_new+window_x\n",
    "            \n",
    "            if i_limit_new >= height:\n",
    "                i_limit_new = height-1\n",
    "            if j_limit_new >= width:\n",
    "                j_limit_new = width-1\n",
    "            \n",
    "            patch = image[i_new:i_limit_new, j_new:j_limit_new]\n",
    "            plt.imshow(patch, interpolation='nearest')\n",
    "            plt.show()\n",
    "            \n",
    "#             res = image_id + ' ' + str(np.max(predicted_probabilities[locations[k]][k])) + ' ' + str(j_new) + ' ' + str(i_new) + ' ' + str(j_limit_new) + ' ' + str(i_limit_new) + '\\n'\n",
    "#             print(res)\n",
    "#             if k == 0:\n",
    "#                 wa.write(res)\n",
    "#             if k == 1:\n",
    "#                 we.write(res)\n",
    "#             if k == 2:\n",
    "#                 wi.write(res)\n",
    "        break\n",
    "        image_id = f.readline().rstrip()\n",
    "\n",
    "\n",
    "sliding_window(200, 600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
